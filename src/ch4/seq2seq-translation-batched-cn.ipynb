{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# 使用seq2seq网络和注意力机制实现汉-英翻译\n",
    "\n",
    "这个项目，我们会实现一个汉语到英语的神经网络翻译模型。\n",
    "\n",
    "\n",
    "这里实现的是论文[seq2seq 网络](http://arxiv.org/abs/1409.3215)的系统，它通过两个RNN来实现把一个训练翻译成两位一个序列。Encoder把输入序列编码成一个向量，而Decoder会把这个向量解码成两位一个输出序列。\n",
    "\n",
    "为了提升效果，我们使用了[注意力机制](https://arxiv.org/abs/1409.0473)，它可以让decoder学习到在翻译的过程中怎么重点关注输入序列的某一部分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sequence to sequence模型\n",
    "\n",
    "sequence to sequence模型，或者说seq2seq模型，由两个RNN组成。这两个RNN分别叫做encoder和decoder。\n",
    "encoder会一个词一个出的读入输入序列，每一步都有一个输出，而最后的输出叫做context向量，我们可以认为是模型对源语言句子“语义”的一种表示。而decoder用这个context向量一步一步的生成目标语言的句子。\n",
    "\n",
    "![](seq2seq.png)\n",
    "\n",
    "为什么要两个RNN呢，如果我们使用一个RNN，输入和输出是一对一的关系（对于分类，我们可以只使用最后一个输出），但是翻译肯定不是一个词对一个词的翻译。当然这只是使用两个RNN在形式上的方便，从“原理”上来说，人类翻译也是类似的，首先仔细阅读源语句，然后“理解”它，而所谓的“理解”在seq2seq模型里可以认为encoding的过程，然后再根据理解，翻译成目标语句。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意力机制\n",
    "\n",
    "用一个固定长度的向量来承载输入序列的完整“语义”，不管向量能有多长，都是很困难的事情。\n",
    "\n",
    "[Bahdanau et al.等人引入的](https://arxiv.org/abs/1409.0473) **注意力机制**试图这样来解决这个问题：我们不依赖于一个固定长度的向量，而是通过“注意”输入的某些部分。在decoer每一步解码的时候都通过这个机制来选择输入的一部分来重点考虑。这似乎是合乎人类的翻译过程的——我们首先通过一个encoder大致理解句子的意思（编码到一个定长向量），具体翻译某个词或者短语的时候我们会仔细推敲对应的源语言的词（注意力机制）。\n",
    "\n",
    "![](5y6SCvU.png)\n",
    "\n",
    "注意力是通过decoder的另外一个神经网络层来计算的。它的输入是当前输入和上一个时刻的隐状态，输出是一个新的向量，这个向量的长度和输入相同（因为输入是变长的，我们会选择一个最大的长度），这个向量会用softmax变成“概率”，得到*注意力权重*，这个权重可以认为需要花费多大的“注意力”到某个输入上，因此我们会用这个权重加权平均encoder的输出，从而得到一个新的context向量，这个向量会用来预测当前时刻的输出。\n",
    "\n",
    "![](K1qMPxs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依赖\n",
    "\n",
    "我们需要[PyTorch](http://pytorch.org/)来构建和训练模型，需要[matplotlib](https://matplotlib.org/) 来绘图，剩下的都是Python自带的库。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import math\n",
    "import socket\n",
    "hostname = socket.gethostname()\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence#, masked_cross_entropy\n",
    "from masked_cross_entropy import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有GPU，请设置为False，但是训练时间可能会比较长。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据\n",
    "\n",
    "训练数据是几千个英语到汉语的平行句对。我们这里只是介绍算法，所以使用一个很小的数据集来演示。数据在data/eng-chn.txt里，格式如下：\n",
    "```\n",
    "I won!  我 赢 了 。\n",
    "```\n",
    "\n",
    "每一行都是一个句对，用tab分割，中文部分已经提前分完词了(空格分开，所以split就可以得到中文的词)。\n",
    "\n",
    "我们会用one-hot的方法来表示一个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单词变成数字\n",
    "\n",
    "我们会创建一个Lang对象来表示源/目标语言，它包含word2idx、idx2word和word2count，分别表示单词到id、id到单词和单词的词频。\n",
    "word2count的作用是用于过滤一些低频词（把它变成unknown）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#按字分词，目前不需要了。\n",
    "def seg_cn_words(s):\n",
    "    words=[]\n",
    "    for ch in s:\n",
    "        if ch !=' ':\n",
    "            words.append(ch)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_token = 0\n",
    "SOS_token = 1\n",
    "EOS_token = 2\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # Count default tokens\n",
    "\n",
    "    def seg_words(self, sentence):\n",
    "#        if self.name=='chn':\n",
    "#            return seg_cn_words(sentence)\n",
    "        return sentence.split(' ')\n",
    "        \n",
    "    def index_words(self, sentence):\n",
    "        words=self.seg_words(sentence)\n",
    "        for word in words:\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # 删除掉频率少于阈值的此。\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed: return\n",
    "        self.trimmed = True\n",
    "        \n",
    "        keep_words = []\n",
    "        \n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words %s / %s = %.4f' % (\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # 重新构建dict\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"PAD\", 1: \"SOS\", 2: \"EOS\"}\n",
    "        self.n_words = 3 # 默认3个词。\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.index_word(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取和预处理 \n",
    "\n",
    "我们会只保留文本里的英文和中文字符，把大小转换成小写。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# 大写转小写，trim，移除常见标点，英文和中文之外的字符。\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([,.!?])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"[^a-zA-Z\\u4e00-\\u9fa5,.!?，。！？]+\", r\" \", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文word  cn!\n",
      "中文word cn !\n"
     ]
    }
   ],
   "source": [
    "s=unicode_to_ascii(\"中文word  cn!\")\n",
    "print(s)\n",
    "s=normalize_string(s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件格式是每行一个句对，用tab分隔，第一个是英语，第二个是汉语，为了方便未来的复用，我们有一个reverse参数，这样如果我们需要汉语到英语的翻译就可以用到。如果读者想实现英语到汉英的翻译系统，那么只需要reverse为False就行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_langs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # 读取文件\n",
    "    filename = '../data/%s-%s.txt' % (lang1, lang2)\n",
    "    lines = open(filename).read().strip().split('\\n')\n",
    "\n",
    "    # Split\n",
    "    pairs = [[normalize_string(s) for s in l.split('\\t')] for l in lines]\n",
    "\n",
    "    # 反转句对，从英汉变成汉英\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_LENGTH = 3\n",
    "MAX_LENGTH = 25\n",
    "\n",
    "def filter_pairs(pairs):\n",
    "    filtered_pairs = []\n",
    "    for pair in pairs:\n",
    "        if len(pair[0]) >= MIN_LENGTH and len(pair[0]) <= MAX_LENGTH \\\n",
    "            and len(pair[1]) >= MIN_LENGTH and len(pair[1]) <= MAX_LENGTH:\n",
    "                filtered_pairs.append(pair)\n",
    "    return filtered_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据处理过程如下：\n",
    "\n",
    "* 读取文件，split成行，再split成pair\n",
    "* 文本归一化，过滤掉过短(小于3)和过长(大于25)的句对。\n",
    "* 通过pair里的句子得到单词列表\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 19777 sentence pairs\n",
      "Filtered to 5095 pairs\n",
      "Indexing words...\n",
      "Indexed 2930 words in input language, 2258 words in output\n"
     ]
    }
   ],
   "source": [
    "def prepare_data(lang1_name, lang2_name, reverse=False):\n",
    "    input_lang, output_lang, pairs = read_langs(lang1_name, lang2_name, reverse)\n",
    "    print(\"Read %d sentence pairs\" % len(pairs))\n",
    "    \n",
    "    pairs = filter_pairs(pairs)\n",
    "    print(\"Filtered to %d pairs\" % len(pairs))\n",
    "    \n",
    "    print(\"Indexing words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.index_words(pair[0])\n",
    "        output_lang.index_words(pair[1])\n",
    "    \n",
    "    print('Indexed %d words in input language, %d words in output' % (input_lang.n_words, output_lang.n_words))\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "input_lang, output_lang, pairs = prepare_data('eng', 'chn', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤词汇 \n",
    "\n",
    "我们会取得词频低于5的词语。对于低频的词和词典之外的词(OOV)，系统很难学习，这是学术研究的一个难点，我们暂时忽略掉这些低频词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keep_words 578 / 2927 = 0.1975\n",
      "keep_words 568 / 2255 = 0.2519\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 5\n",
    "\n",
    "input_lang.trim(MIN_COUNT)\n",
    "output_lang.trim(MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chn\n"
     ]
    }
   ],
   "source": [
    "print(input_lang.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过滤句对\n",
    "\n",
    "我们接下来去掉包含低频词或者OOV的句对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 5095 pairs to 1854, 0.3639 of total\n"
     ]
    }
   ],
   "source": [
    "keep_pairs = []\n",
    "\n",
    "for pair in pairs:\n",
    "    input_sentence = pair[0]\n",
    "    output_sentence = pair[1]\n",
    "    keep_input = True\n",
    "    keep_output = True\n",
    "    words=input_lang.seg_words(input_sentence)\n",
    "    for word in words:\n",
    "        if word not in input_lang.word2index:\n",
    "            keep_input = False\n",
    "            break\n",
    "            \n",
    "    words=output_lang.seg_words(output_sentence)\n",
    "    for word in words:\n",
    "        if word not in output_lang.word2index:\n",
    "            keep_output = False\n",
    "            break\n",
    "\n",
    "    # Remove if pair doesn't match input and output conditions\n",
    "    if keep_input and keep_output:\n",
    "        keep_pairs.append(pair)\n",
    "\n",
    "print(\"Trimmed from %d pairs to %d, %.4f of total\" % (len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "pairs = keep_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我 赢 了 。', 'i won !']\n"
     ]
    }
   ],
   "source": [
    "print(pairs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把训练数据变成Tensor/Variable\n",
    "\n",
    "为了让神经网络能够处理，我们首先需要把句子变成Tensor。每个句子首先分成词，每个词被替换成对应的index。另外我们会增加一个特殊的EOS来表示句子的结束。\n",
    "\n",
    "![](https://i.imgur.com/LzocpGH.png)\n",
    "\n",
    "在PyTorch里一个Tensor是一个多维数组，它的所有元素的数据类型都是一样的。我们这里使用LongTensor来表示词的index。\n",
    "\n",
    "可以训练的PyTorch模块要求输入是Variable而不是Tensor。变量除了包含Tensor的内容之外，它还会跟踪计算图的状态，从而可以进行自动梯度的求值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把句子变成index的list，最后加上EOS\n",
    "def indexes_from_sentence(lang, sentence):\n",
    "    idxs=[]\n",
    "    words=lang.seg_words(sentence)\n",
    "    for word in words:\n",
    "        if word in lang.word2index:\n",
    "            idxs.append(lang.word2index[word])\n",
    "    idxs.append(EOS_token)\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更好的使用GPU，我们一次训练一个batch的数据，但是问题是不同句对的长度是不一样的。解决办法是通过“pad”来把短的序列补到和长的序列一样长，我们需要一个特殊的整数来表示它是一个pad值而不是其他的词，我们这里使用0。当计算loss的时候，我们也需要把这些0对应的loss去掉（因为实际的序列到这里已经没有了，但是padding之后任何会计算出模型的预测值，从而有loss）\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/gGlkEEF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把一个序列padding到长度为max_length。\n",
    "def pad_seq(seq, max_length):\n",
    "    seq += [PAD_token for i in range(max_length - len(seq))]\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练一次需要一个batch的数据，我们一般随机的抽取bach_size个句对，然后把它们padding到最长的序列长度。此外，我们需要记录下未padding之前的长度，因为计算loss的时候会需要用到它。\n",
    "\n",
    "我们会用list的list来初始化LongTensor，这个list的大小是batch_size，list的每个元素是一个序列(句子)。这样我们可以得到一个`(batch_size x max_len)`的Tensor，但是因为训练时我们需要逐个时刻的计算batch_size个数据，所以我们需要把它转置成`(max_len x batch_size)`。这样tensor[t]就表示t时刻的batch_size个词。\n",
    "\n",
    "\n",
    "![](https://i.imgur.com/nBxTG3v.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size):\n",
    "    input_seqs = []\n",
    "    target_seqs = []\n",
    "\n",
    "    # 随机选择pairs\n",
    "    for i in range(batch_size):\n",
    "        pair = random.choice(pairs)\n",
    "        input_seqs.append(indexes_from_sentence(input_lang, pair[0]))\n",
    "        target_seqs.append(indexes_from_sentence(output_lang, pair[1]))\n",
    "\n",
    "    # 把输入和输出序列zip起来，通过输入的长度降序排列，然后unzip\n",
    "    seq_pairs = sorted(zip(input_seqs, target_seqs), key=lambda p: len(p[0]), reverse=True)\n",
    "    input_seqs, target_seqs = zip(*seq_pairs)\n",
    "    \n",
    "    # 对输入和输出序列都进行padding。\n",
    "    input_lengths = [len(s) for s in input_seqs]\n",
    "    input_padded = [pad_seq(s, max(input_lengths)) for s in input_seqs]\n",
    "    target_lengths = [len(s) for s in target_seqs]\n",
    "    target_padded = [pad_seq(s, max(target_lengths)) for s in target_seqs]\n",
    "\n",
    "    # padding之后的shape是(batch_size x max_len)，我们需要把它转置成(max_len x batch_size)\n",
    "    input_var = Variable(torch.LongTensor(input_padded)).transpose(0, 1)\n",
    "    target_var = Variable(torch.LongTensor(target_padded)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_var = input_var.cuda()\n",
    "        target_var = target_var.cuda()\n",
    "        \n",
    "    return input_var, input_lengths, target_var, target_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以测试一下，它会返回两个`(max_len x batch_size)` tensor，这是输入和输出序列，同时也会返回padding之前的实际长度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 19, 156],\n",
       "         [ 11,  30],\n",
       "         [173, 480],\n",
       "         [ 55, 481],\n",
       "         [ 72,   3],\n",
       "         [133,   2],\n",
       "         [ 26,   0],\n",
       "         [  2,   0]], device='cuda:0'), [8, 6], tensor([[ 18,  99],\n",
       "         [274, 140],\n",
       "         [ 25, 101],\n",
       "         [144, 476],\n",
       "         [ 17,   3],\n",
       "         [ 21,   2],\n",
       "         [  2,   0]], device='cuda:0'), [7, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "<img src=\"images/encoder-network.png\" style=\"float: right\" />\n",
    "\n",
    "\n",
    "encoder的输入是batch大小的词序列——大小是`(max_len x batch_size)``LongTensor`，每个词都会输出一个向量，所以最终的输出是大小为  `(max_len x batch_size x hidden_size)`的`FloatTensor`。\n",
    "\n",
    "输入序列的词会被fed到一个[embedding层 `nn.Embedding`](http://pytorch.org/docs/nn.html#embedding)从而得到每个词的embedding，这样得到一个`(max_len x batch_size x embedding_size【这里embedding_size==hidden_size，但这不是必须相等的】)`的Tensor，这个Tensor会输入到[GRU层 `nn.GRU`](http://pytorch.org/docs/nn.html#gru)。最终GRU的输出是`seq_len x batch_size x hidden_size`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1, dropout=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=self.dropout, bidirectional=True)\n",
    "        \n",
    "    def forward(self, input_seqs, input_lengths, hidden=None):\n",
    "        # 注意：和前面的实现不同，这里没有时刻t的for循环，而是一次输入GRU直接计算出来\n",
    "        embedded = self.embedding(input_seqs)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack\n",
    "        hidden = self._cat_directions(hidden)\n",
    "        return outputs, hidden\n",
    "    \n",
    "    def _cat_directions(self, hidden):\n",
    "        \"\"\" If the encoder is bidirectional, do the following transformation.\n",
    "            Ref: https://github.com/IBM/pytorch-seq2seq/blob/master/seq2seq/models/DecoderRNN.py#L176\n",
    "            -----------------------------------------------------------\n",
    "            In: (num_layers * num_directions, batch_size, hidden_size)\n",
    "            (ex: num_layers=2, num_directions=2)\n",
    "\n",
    "            layer 1: forward__hidden(1)\n",
    "            layer 1: backward_hidden(1)\n",
    "            layer 2: forward__hidden(2)\n",
    "            layer 2: backward_hidden(2)\n",
    "\n",
    "            -----------------------------------------------------------\n",
    "            Out: (num_layers, batch_size, hidden_size * num_directions)\n",
    "\n",
    "            layer 1: forward__hidden(1) backward_hidden(1)\n",
    "            layer 2: forward__hidden(2) backward_hidden(2)\n",
    "        \"\"\"\n",
    "        def _cat(h):\n",
    "            return torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
    "            \n",
    "        if isinstance(hidden, tuple):\n",
    "            # LSTM hidden contains a tuple (hidden state, cell state)\n",
    "            hidden = tuple([_cat(h) for h in hidden])\n",
    "        else:\n",
    "            # GRU hidden\n",
    "            hidden = _cat(hidden)\n",
    "            \n",
    "        return hidden    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 理解Bahdanau等人提出的模型\n",
    "\n",
    "下面我们来学习一下这篇文章提出的 [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473) Attention Decoder。\n",
    "\n",
    "decoder在每一个时刻的输出依赖与前一个时刻的输出和一个$\\mathbf x$，这个$\\mathbf x$包括当前的隐状态（它也会考虑前一个时刻的输出）和一个注意力”context“，下文会介绍它。函数$g$是一个带非线性激活的全连接层，它的输入是$y_{i-1}$, $s_i$ 和 $c_i$拼接起来的。\n",
    "\n",
    "$$\n",
    "p(y_i \\mid \\{y_1,...,y_{i-1}\\},\\mathbf{x}) = g(y_{i-1}, s_i, c_i)\n",
    "$$\n",
    "\n",
    "上式的意思是：我们在翻译当前词时只考虑上一个翻译出来的词以及当前的隐状态和注意力context。\n",
    "\n",
    "当前隐状态$s_i$是有RNN $f$计算出来的，这个RNN的输入是上一个隐状态$s_{i-1}$，decoder的上一个输出$y_{i-1}$和 context向量$c_i$。\n",
    "\n",
    "在代码实现中，我们使用的RNN是`nn.GRU`，隐状态 $s_i$是`hidden`，输出$y_i$是`output`， context $c_i$ 是`context`。\n",
    "\n",
    "$$\n",
    "s_i = f(s_{i-1}, y_{i-1}, c_i)\n",
    "$$\n",
    "\n",
    "context向量$c_i$是encoder在每个时刻(词)的输出的加权和，而权值$a_{ij}$表示i时刻需要关注$h_j$的程度(概率)。\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{j=1}^{T_x} a_{ij} h_j\n",
    "$$\n",
    "\n",
    "而权值$a_{ij}$是\"能量\" $e_{ij}$的softmax。\n",
    "\n",
    "$$\n",
    "a_{ij} = \\dfrac{exp(e_{ij})}{\\sum_{k=1}^{T} exp(e_{ik})}\n",
    "$$\n",
    "\n",
    "而能量$e_{ij}$是上个时刻的隐状态$s_{i-1}$和encoder第j个时刻的输出$h_j$的函数：\n",
    "\n",
    "$$\n",
    "e_{ij} = a(s_{i-1}, h_j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luong等人提出的模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luong等人在[Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)提出了更多的提高和简化。他们描述了”全局注意力“模型，其计算注意力得分的方法和之前不同。前面是通过$s_{i-1}$和$h_j$计算$a_{ij}$，也就是当前的注意力权重依赖与前一个状态，而这里的注意力依赖与decoder当前的隐状态和encoder所有隐状态：\n",
    "\n",
    "$$\n",
    "a_t(s) = align(h_t, \\bar h_s)  = \\dfrac{exp(score(h_t, \\bar h_s))}{\\sum_{s'} exp(score(h_t, \\bar h_{s'}))}\n",
    "$$\n",
    "\n",
    "特点的\"score\"函数会比较两个隐状态的”相似度“，可以是两个向量的内积，也可以是$h_{s'}$做一个线性变换之后和$h_t$的内积，也可以是把两个向量拼接起来然后做一个线性变换，然后和一个参数$v_a$（这个参数是学习出来的）的内积：\n",
    "\n",
    "$$\n",
    "score(h_t, \\bar h_s) =\n",
    "\\begin{cases}\n",
    "h_t ^\\top \\bar h_s & dot \\\\\n",
    "h_t ^\\top \\textbf{W}_a \\bar h_s & general \\\\\n",
    "v_a ^\\top \\textbf{W}_a [ h_t ; \\bar h_s ] & concat\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "scoring函数的模块化定义使得我们可以随意的修改而不影响其它地方的代码。这个模块的输入总是decoder的隐状态和encoder的所有输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算注意力的通用模块\n",
    "\n",
    "为了代码重用，这里把注意力的计算封装成一个模块，不管是哪个算法都可以复用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        \n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size*2, hidden_size*2)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(0)\n",
    "        this_batch_size = encoder_outputs.size(1)\n",
    "\n",
    "        # 创建变量来存储注意力能量\n",
    "        attn_energies = Variable(torch.zeros(this_batch_size, max_len)) # B x S\n",
    "\n",
    "        if USE_CUDA:\n",
    "            attn_energies = attn_energies.cuda()\n",
    "\n",
    "        # 计算\n",
    "        for b in range(this_batch_size):\n",
    "            for i in range(max_len):\n",
    "                attn_energies[b, i] = self.score(hidden[:, b], encoder_outputs[i, b].unsqueeze(0))\n",
    "\n",
    "        # softmax并且resize成1 x B x S\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "    \n",
    "        \n",
    "    def score(self, hidden, encoder_output):\n",
    "\n",
    "        if self.method == 'dot':\n",
    "            energy = hidden.view(-1).dot(encoder_output.view(-1))\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'general':\n",
    "            energy = self.attn(encoder_output)\n",
    "            energy = hidden.view(-1).dot(energy.view(-1))\n",
    "            return energy\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            energy = self.attn(torch.cat((hidden, encoder_output), 1))\n",
    "            energy = self.other.view(-1).dot(energy.view(-1))\n",
    "            return energy        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们可以构建一个decoder，它会把Attn模块放到RNN之后用来计算注意力的权重，并且用它来计算context向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # 保存变量\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # 定义网络层\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size*2, n_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size * 4, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # 选择注意力计算方法\n",
    "        if attn_model != 'none':\n",
    "            self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_seq, last_hidden, encoder_outputs):\n",
    "        # 注意：我们encoder一次计算所有时刻的数据，但是decoder我们目前还是一次计算一个时刻的（但是是一个batch）\n",
    "        # 因为Teacher Forcing可以一次计算但是Random Sample必须逐个计算\n",
    "        # 得到当前输入的embedding \n",
    "        batch_size = input_seq.size(0)\n",
    "        embedded = self.embedding(input_seq)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "        embedded = embedded.view(1, batch_size, self.hidden_size) # S=1 x B x N\n",
    "\n",
    "        # 计算gru的输出和新的隐状态，输入是当前词的embedding和之前的隐状态。\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        \n",
    "        \n",
    "        # 根据当前的RNN状态和encoder的输出计算注意力。\n",
    "        # 根据注意力计算context向量 \n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1)) # B x S=1 x N\n",
    "\n",
    "        # 把gru的输出和context vector拼接起来\n",
    "        rnn_output = rnn_output.squeeze(0) # S=1 x B x N -> B x N\n",
    "        context = context.squeeze(1)       # B x S=1 x N -> B x N\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        # 预测下一个token，这里没有softmax，只有计算loss的时候才需要。\n",
    "        output = self.out(concat_output)\n",
    "\n",
    "        # 返回最终的输出，GRU的隐状态和attetion（用于可视化）\n",
    "        return output, hidden, attn_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试模型\n",
    "\n",
    "为了初步验证上面的代码是否正确，我们用少量的数据来跑一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batches torch.Size([5, 3])\n",
      "target_batches torch.Size([6, 3])\n"
     ]
    }
   ],
   "source": [
    "small_batch_size = 3\n",
    "input_batches, input_lengths, target_batches, target_lengths = random_batch(small_batch_size)\n",
    "\n",
    "print('input_batches', input_batches.size()) # (max_len x batch_size)\n",
    "print('target_batches', target_batches.size()) # (max_len x batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个较小的模型用来验证代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_hidden_size = 8\n",
    "small_n_layers = 2\n",
    "\n",
    "encoder_test = EncoderRNN(input_lang.n_words, small_hidden_size, small_n_layers)\n",
    "decoder_test = LuongAttnDecoderRNN('general', small_hidden_size, output_lang.n_words, small_n_layers)\n",
    "\n",
    "if USE_CUDA:\n",
    "    encoder_test.cuda()\n",
    "    decoder_test.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_outputs torch.Size([5, 3, 16])\n",
      "encoder_hidden torch.Size([2, 3, 16])\n"
     ]
    }
   ],
   "source": [
    "encoder_outputs, encoder_hidden = encoder_test(input_batches, input_lengths, None)\n",
    "\n",
    "print('encoder_outputs', encoder_outputs.size()) # max_len x batch_size x hidden_size\n",
    "print('encoder_hidden', encoder_hidden.size()) # n_layers * 2 x batch_size x hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后我们从SOS开始，然后用decoder来逐个解码。和encoder不同，encoder是一次处理整个序列，但是decoder必须逐个解码，因为在预测（或者random sampling时）都必须把上一个时刻的输出作为当前时刻的输入。所以前一个时刻没有处理完是没有办法处理下一个时刻的数据的。不过我们仍然是一次处理一个batch的数据的，比如之前的效率还是会提高。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 6.273172378540039\n"
     ]
    }
   ],
   "source": [
    "max_target_length = max(target_lengths)\n",
    "\n",
    "# decoder的第一个输入是SOS\n",
    "decoder_input = Variable(torch.LongTensor([SOS_token] * small_batch_size))\n",
    "decoder_hidden = encoder_hidden # 把第一层的encoder的输出作为decoder的输入。\n",
    "all_decoder_outputs = Variable(torch.zeros(max_target_length, small_batch_size, decoder_test.output_size))\n",
    "\n",
    "if USE_CUDA:\n",
    "    all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "    decoder_input = decoder_input.cuda()\n",
    "\n",
    "# 遍历Run through decoder one time step at a time\n",
    "for t in range(max_target_length):\n",
    "    decoder_output, decoder_hidden, decoder_attn = decoder_test(\n",
    "        decoder_input, decoder_hidden, encoder_outputs\n",
    "    )\n",
    "    all_decoder_outputs[t] = decoder_output # Store this step's outputs\n",
    "    decoder_input = target_batches[t] # Next input is current target\n",
    "\n",
    "# 通过mask计算交叉熵\n",
    "loss = masked_cross_entropy(\n",
    "    all_decoder_outputs.transpose(0, 1).contiguous(),\n",
    "    target_batches.transpose(0, 1).contiguous(),\n",
    "    target_lengths,\n",
    "    USE_CUDA\n",
    ")\n",
    "print('loss', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 训练\n",
    "\n",
    "### 一次训练\n",
    "\n",
    "对于一个训练数据，我们首先用encoder对输入句子进行编码，得到每个时刻的输出和最后一个时刻的隐状态。最后一个隐状态会作为decoder隐状态的初始值，并且我们会用一个特殊的`<SOS>`作为decoder的第一个输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_batches, input_lengths, target_batches, target_lengths, encoder, decoder, \n",
    "          encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    \n",
    "    # 梯度清空\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0 # Added onto for each word\n",
    "\n",
    "    # 进行encoding\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "    \n",
    "    # 准备输入和输出变量\n",
    "    decoder_input = Variable(torch.LongTensor([SOS_token] * batch_size))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    #decoder_hidden = decoder_hidden.contiguous()\n",
    "    \n",
    "    max_target_length = max(target_lengths)\n",
    "    all_decoder_outputs = Variable(torch.zeros(max_target_length, batch_size, decoder.output_size))\n",
    "\n",
    "\n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "        all_decoder_outputs = all_decoder_outputs.cuda()\n",
    "\n",
    "    # 解码\n",
    "    for t in range(max_target_length):\n",
    "        decoder_output, decoder_hidden, decoder_attn = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "\n",
    "        all_decoder_outputs[t] = decoder_output\n",
    "        decoder_input = target_batches[t] # 当前输出是下一个时刻的输入。\n",
    "\n",
    "    # 计算loss和反向计算梯度\n",
    "    loss = masked_cross_entropy(\n",
    "        all_decoder_outputs.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_batches.transpose(0, 1).contiguous(), # -> batch x seq\n",
    "        target_lengths,\n",
    "        USE_CUDA\n",
    "    )\n",
    "    loss.backward()\n",
    "    \n",
    "    # clip梯度\n",
    "    ec = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    dc = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # 更新参数\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.item(), ec, dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行训练\n",
    "\n",
    "所有的准备工作都就绪，我们可以开始初始化网络和开始训练。\n",
    "\n",
    "我们首先初始化模型，优化器，损失函数，以及一些用于绘图和记录进度的变量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的配置\n",
    "attn_model = 'dot'\n",
    "hidden_size = 500\n",
    "n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 100\n",
    "\n",
    "# 训练的超参数\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_epochs = 5000\n",
    "epoch = 0\n",
    "plot_every = 20\n",
    "print_every = 100\n",
    "evaluate_every = 500\n",
    "\n",
    "# 初始化模型\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers, dropout=dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, hidden_size, output_lang.n_words, n_layers, dropout=dropout)\n",
    "\n",
    "# 初始化optimizers和criterion\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 把模型放到GPU上\n",
    "if USE_CUDA:\n",
    "    encoder.cuda()\n",
    "    decoder.cuda()\n",
    "\n",
    "# 计时\n",
    "start = time.time()\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # 每过print_every次清零\n",
    "plot_loss_total = 0 # 每过plot_every次清零"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_minutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def time_since(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (as_minutes(s), as_minutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预测(翻译)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_seq, max_length=MAX_LENGTH):\n",
    "    input_seqs = [indexes_from_sentence(input_lang, input_seq)]\n",
    "    input_lengths=[len(sen) for sen in input_seqs]\n",
    "    with torch.no_grad():\n",
    "        input_batches = Variable(torch.LongTensor(input_seqs)).transpose(0, 1)\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        input_batches = input_batches.cuda()\n",
    "        \n",
    "    # 预测的时候不需要更新参数\n",
    "    encoder.train(False)\n",
    "    decoder.train(False)\n",
    "    \n",
    "    # encoding\n",
    "    encoder_outputs, encoder_hidden = encoder(input_batches, input_lengths, None)\n",
    "\n",
    "    # 创建开始的SOS\n",
    "    with torch.no_grad():\n",
    "        decoder_input = Variable(torch.LongTensor([SOS_token])) # SOS\n",
    "    decoder_hidden = encoder_hidden\n",
    "    #decoder_hidden = decoder_hidden.contiguous()\n",
    "    \n",
    "    if USE_CUDA:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Store output words and attention states\n",
    "    decoded_words = []\n",
    "    decoder_attentions = torch.zeros(max_length + 1, max_length + 1)\n",
    "    \n",
    "    # Run through decoder\n",
    "    for di in range(max_length):\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        decoder_attentions[di,:decoder_attention.size(2)] += decoder_attention.squeeze(0).squeeze(0).cpu().data\n",
    "\n",
    "        # Choose top word from output\n",
    "        topv, topi = decoder_output.data.topk(1)\n",
    "        ni = topi.item()\n",
    "        if ni == EOS_token:\n",
    "            decoded_words.append('<EOS>')\n",
    "            break\n",
    "        else:\n",
    "            decoded_words.append(output_lang.index2word[ni])\n",
    "            \n",
    "        # Next input is chosen word\n",
    "        decoder_input = Variable(torch.LongTensor([ni]))\n",
    "        if USE_CUDA: decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Set back to training mode\n",
    "    encoder.train(True)\n",
    "    decoder.train(True)\n",
    "    \n",
    "    return decoded_words, decoder_attentions[:di+1, :len(encoder_outputs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "我们可以随机选些训练集中的句子来看模型的预测和参考翻译的差异："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_randomly():\n",
    "    [input_sentence, target_sentence] = random.choice(pairs)\n",
    "    evaluate_and_show_attention(input_sentence, target_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 注意力的可视化\n",
    "\n",
    "注意力机制的一个优点是它的输出的可解释性。因为它在decode每一步都会对所以输入赋予不同的权重，因此我们可以认为它在翻译这个词的时候”对应“了源语言的词的权重，这可以认为是一种对齐。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Setting up a new session...\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import visdom\n",
    "vis = visdom.Visdom()\n",
    "\n",
    "def show_plot_visdom():\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    attn_win = 'attention (%s)' % hostname\n",
    "    vis.image(torchvision.transforms.ToTensor()(Image.open(buf)), win=attn_win, opts={'title': attn_win})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了更好的可视化效果，我们下面设置colorbar和axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    words=input_lang.seg_words(input_sentence)\n",
    "    ax.set_xticklabels([''] + words + ['<EOS>'])\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    show_plot_visdom()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_show_attention(input_sentence, target_sentence=None):\n",
    "    output_words, attentions = evaluate(input_sentence)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    print('>', input_sentence)\n",
    "    if target_sentence is not None:\n",
    "        print('=', target_sentence)\n",
    "    print('<', output_sentence)\n",
    "    \n",
    "    show_attention(input_sentence, output_words, attentions)\n",
    "    \n",
    "    # Show input, target, output text in visdom\n",
    "    win = 'evaluted (%s)' % hostname\n",
    "    text = '<p>&gt; %s</p><p>= %s</p><p>&lt; %s</p>' % (input_sentence, target_sentence, output_sentence)\n",
    "    vis.text(text, win=win, opts={'title': win})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 放到一起\n",
    "\n",
    "接下来就是开始训练了，注意需要在先运行 python -m visdom.server"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 55s (- 1961m 38s) (100 0%) 3.1943\n",
      "7m 43s (- 1923m 15s) (200 0%) 1.6655\n",
      "11m 16s (- 1867m 15s) (300 0%) 0.8233\n",
      "14m 51s (- 1843m 19s) (400 0%) 0.3746\n",
      "18m 36s (- 1841m 41s) (500 1%) 0.2056\n",
      "22m 7s (- 1821m 22s) (600 1%) 0.1132\n",
      "25m 43s (- 1811m 21s) (700 1%) 0.0767\n",
      "29m 17s (- 1801m 29s) (800 1%) 0.0648\n",
      "32m 45s (- 1787m 6s) (900 1%) 0.0492\n",
      "36m 19s (- 1779m 38s) (1000 2%) 0.0421\n",
      "> 他 今天 见 他们 了 吗 ?\n",
      "= has he met them today ?\n",
      "< has he met them today ? <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39m 56s (- 1775m 32s) (1100 2%) 0.0371\n",
      "43m 31s (- 1769m 46s) (1200 2%) 0.0336\n",
      "47m 7s (- 1765m 37s) (1300 2%) 0.0319\n",
      "50m 35s (- 1756m 25s) (1400 2%) 0.0309\n",
      "54m 4s (- 1748m 14s) (1500 3%) 0.0305\n",
      "57m 29s (- 1739m 20s) (1600 3%) 0.0287\n",
      "61m 1s (- 1733m 58s) (1700 3%) 0.0279\n",
      "64m 27s (- 1726m 7s) (1800 3%) 0.0262\n",
      "67m 58s (- 1721m 0s) (1900 3%) 0.0424\n",
      "71m 26s (- 1714m 28s) (2000 4%) 0.0593\n",
      "> 那 是 我们 的 房子 。\n",
      "= that s our house .\n",
      "< that s our house . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74m 52s (- 1707m 50s) (2100 4%) 0.0314\n",
      "78m 22s (- 1702m 56s) (2200 4%) 0.0248\n",
      "81m 49s (- 1697m 2s) (2300 4%) 0.0248\n",
      "85m 18s (- 1691m 59s) (2400 4%) 0.0255\n",
      "88m 48s (- 1687m 24s) (2500 5%) 0.0237\n",
      "92m 19s (- 1683m 8s) (2600 5%) 0.0243\n",
      "95m 53s (- 1679m 50s) (2700 5%) 0.0228\n",
      "99m 26s (- 1676m 16s) (2800 5%) 0.0233\n",
      "102m 56s (- 1672m 2s) (2900 5%) 0.0234\n",
      "106m 35s (- 1669m 58s) (3000 6%) 0.0216\n",
      "> 她 为 甚 么 要 说谎 ?\n",
      "= why would she lie ?\n",
      "< why would she lie ? <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110m 7s (- 1666m 4s) (3100 6%) 0.0225\n",
      "113m 40s (- 1662m 28s) (3200 6%) 0.0232\n",
      "117m 10s (- 1658m 13s) (3300 6%) 0.0228\n",
      "120m 38s (- 1653m 23s) (3400 6%) 0.0281\n",
      "124m 8s (- 1649m 21s) (3500 7%) 0.0496\n",
      "127m 43s (- 1646m 10s) (3600 7%) 0.0471\n",
      "131m 19s (- 1643m 19s) (3700 7%) 0.0283\n",
      "134m 53s (- 1639m 55s) (3800 7%) 0.0240\n",
      "138m 28s (- 1636m 51s) (3900 7%) 0.0240\n",
      "142m 7s (- 1634m 29s) (4000 8%) 0.0216\n",
      "> 每个 人 都 认识 她 。\n",
      "= everybody knows her .\n",
      "< everybody knows her . <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145m 40s (- 1630m 52s) (4100 8%) 0.0226\n",
      "149m 32s (- 1630m 47s) (4200 8%) 0.0217\n",
      "153m 17s (- 1629m 7s) (4300 8%) 0.0228\n",
      "156m 55s (- 1626m 16s) (4400 8%) 0.0222\n",
      "160m 26s (- 1622m 12s) (4500 9%) 0.0218\n",
      "164m 7s (- 1619m 49s) (4600 9%) 0.0214\n",
      "167m 37s (- 1615m 34s) (4700 9%) 0.0209\n",
      "171m 7s (- 1611m 25s) (4800 9%) 0.0209\n",
      "174m 34s (- 1606m 47s) (4900 9%) 0.0222\n",
      "178m 7s (- 1603m 10s) (5000 10%) 0.0210\n",
      "> 你 怎么 洋 ?\n",
      "= how about you ?\n",
      "< how about you ? <EOS>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181m 42s (- 1599m 49s) (5100 10%) 0.0225\n"
     ]
    }
   ],
   "source": [
    "# Begin!\n",
    "ecs = []\n",
    "dcs = []\n",
    "eca = 0\n",
    "dca = 0\n",
    "\n",
    "while epoch < n_epochs:\n",
    "    epoch += 1\n",
    "    \n",
    "    # Get training data for this cycle\n",
    "    input_batches, input_lengths, target_batches, target_lengths = random_batch(batch_size)\n",
    "\n",
    "    # Run the train function\n",
    "    loss, ec, dc = train(\n",
    "        input_batches, input_lengths, target_batches, target_lengths,\n",
    "        encoder, decoder,\n",
    "        encoder_optimizer, decoder_optimizer, criterion\n",
    "    )\n",
    "\n",
    "    # Keep track of loss\n",
    "    print_loss_total += loss\n",
    "    plot_loss_total += loss\n",
    "    eca += ec\n",
    "    dca += dc\n",
    "\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print_loss_avg = print_loss_total / print_every\n",
    "        print_loss_total = 0\n",
    "        print_summary = '%s (%d %d%%) %.4f' % (time_since(start, epoch / n_epochs), epoch, epoch / n_epochs * 100, print_loss_avg)\n",
    "        print(print_summary)\n",
    "        \n",
    "    if epoch % evaluate_every == 0:\n",
    "        evaluate_randomly()\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        plot_loss_avg = plot_loss_total / plot_every\n",
    "        plot_losses.append(plot_loss_avg)\n",
    "        plot_loss_total = 0\n",
    "        \n",
    "        # TODO: Running average helper\n",
    "        ecs.append(eca / plot_every)\n",
    "        dcs.append(dca / plot_every)\n",
    "        ecs_win = 'encoder grad (%s)' % hostname\n",
    "        dcs_win = 'decoder grad (%s)' % hostname\n",
    "        vis.line(np.array(ecs), win=ecs_win, opts={'title': ecs_win})\n",
    "        vis.line(np.array(dcs), win=dcs_win, opts={'title': dcs_win})\n",
    "        eca = 0\n",
    "        dca = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失绘图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_plot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    loc = ticker.MultipleLocator(base=0.2) # put ticks at regular intervals\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "\n",
    "show_plot(plot_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_words, attentions = evaluate(\"你 爱 音乐 吗 ？\")\n",
    "plt.matshow(attentions.numpy())\n",
    "show_plot_visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_show_attention(\"你 爱 音乐 吗 ？\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "py3.6-env",
   "language": "python",
   "name": "py3.6-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
